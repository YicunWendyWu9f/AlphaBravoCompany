[{"categories":null,"content":"Part 2 of the cheap single node Kubernetes series. In this blog we deploying Longhorn, Monitoring and Logging on RKE2.","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Single-Node Kubernetes Series Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1) Powerful Single Node K3S on Hetzner for CHEAP! (Part 1a) Powerful Single Node K3s on Hetzner for CHEAP! (Part 2) ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:1:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"RKE2 Install In the first blog of this series, we deployed a single node RKE2 deployment and Rancher UI or a cheap Hetzner server. In this post, we will install the following: Longhorn for Persistent Data Rancher Monitoring EFK Logging (Elasticsearch,Fluent Bit,Kibana) ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:2:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"What is Longhorn Longhorn is an official CNCF project that delivers a powerful cloud-native distributed storage platform for Kubernetes that can run anywhere. When combined with Rancher, Longhorn makes the deployment of highly-available, persistent, block storage in your Kubernetes environment easy, fast, and reliable. Some of the key features are: An Inuitive Dashboard Easy 1-Click Deployment Built in Disaster Recovery Tools Infrastructure Agnostic means it run on any Kubernetes, anywhere A CNCF Sandbox Project ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:3:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Longhorn Official Longhorn Documentation As noted in the features above, the install of Longhorn has really been simplified using the one-click installer in the Rancher UI “apps” section. Follow the instructions below to get started: In Rancher MCM, Navigate to the cluster where you will install Longhorn. Navigate to the Apps \u0026 Marketplace, Charts menu item. Find the Longhorn item in the charts and click it. Click Install. Customize the default settings by selecting Customize Helm options before install box. Under Longhorn Storage Class Settings change the Default Storage Class Replica Count from 3 to 1. This is because we are running a single server and Longhorn by default expects 3 separate servers with 3 copies of the data for redundancy. In an actual production cluster, you can leave this at 3. Click Next and Install and Longhorn will install on RKE2. Notes You may want to add additional volumes specifically for Longhorn. To do that, add a volume, format at EXT4 or XFS, and mount it on your host/s. Then you can point to that path in the Longhorn config. In this case, we are just using the additional space on the root volume of the disk as longhorn automaticall mounts to /var/lib/longhorn. You may need to install iscsi-initiator-utils on your system for Longhorn to install properly. ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:4:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Accessing Longhorn Now that Longhorn is installed, we can access the Longhorn UI via the Rancher UI. In Rancher MCM, Navigate to the cluster where installed Longhorn. Navigate to the Longhorn menu item. Click Manage storage system via UI on the Longhorn card and a new window will open with your Longhorn UI showing. Longhorn UI on RKE2\" Longhorn UI on RKE2 ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:5:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Monitoring Now that we have Longhorn installed for Persistent Storage, we can install Monitoring with persistence. The Rancher Monitoring chart will install Prometheus, Grafana, Alert-Manager and other tools to monitor the cluster. Login to the Rancher UI. Go to Apps and Marketplace, Charts. Click on Monitoring menu item and click Install. Leave the version as the default, but select Customize Helm Options before Install at the bottom of the page. Click Next. Under Prometheus (on the inner left column), change the following options: Retention Size: 10GiB CPU Limit: 1500m Memory Limit 2048Mi Select the box for Persistent Storage for Prometheus. Size: 10Gi Storage Class Name: longhorn Under Grafana (on the inner left column), change the following options: Select Enable with PVC Template Size: 10Gi Storage Class Name: longhorn AccessMode: ReadWriteOnce Click Next, then click Install. Once this is complete, you should now have access to monitoring for your node and workloads running in your cluster. ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:6:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Accessing Monitoring Monitoring is complex, especially when it comes to alert configuration.. To get you started, we will show you 2 ways to see what is being monitored and your cluster stats. ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:7:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Via the Workloads In the Rancher UI, click on Workload, Pods. Click on a workload name like longhorn-manager-010101 and then click on Metrics. You can see the different metrics for that specific Pod and change the time scale you ae see it in. You can also click on Grafana to be taken to the full Granfana dashboard for those specific metrics. Workload Metrics UI\" Workload Metrics UI ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:7:1","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Via Grafana In the Rancher UI, click on Monitoring in the menu. Under Grafana, click on Metrics Dashboard. This will open a new tab for Grafana The Welcome screen shows stats for the cluster as a whole. To explore the cluster more, hover on the 4 boxes in the menu and click Manage. This will show you an entire list of metrics you can drill down into and view. Grafana UI\" Grafana UI ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:7:2","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Logging Logging is a large scale endeavor in itself. Make sure you understand the needs of your organization and whether or not you should log to a hosted service (we like LogDNA). However, in many cases, sending logs externally is not an option. In today’s example, we will deploy Elasticsearch locally to ingest the logs, Kibana to view them, and Fluent-bit to ship them from the containers to Elastic. ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Elasticsearch For Elasticsearch and Kibana, we will use the Helm charts maintained by Bitnami, a VMWare entity. They are very well maintained and well documented and are well suited for this application. Login to the Rancher Interface. Go to Apps and Marketplace, Repositories. Click Create and add the following then click Create again. Name: bitnami Index URL: https://charts.bitnami.com/bitnami Under Cluster, click Projects/Namespaces. Under the Default project, click Create Namespace and name it logging-system. Go to Apps and Marketplace, Charts. Select only bitnami from the chart repo options. Click on elasticsearch menu item and click Install. Update the following and click Next: Namespace: logging-system Name: elasticsearch-logging Customize Helm options before install: check. Replace all the YAML with the below code. In the below code, update the elasticPassword: with your desired password. Make this complex because it is complicated to change. Save this password as it will be required for Kibana and Fluent. Expand The Code In the next few sections, the code blocks are quite large. We are showing them and collapsed in to to make this doc easier to read. In order to view/copy the code, click the arrow next to YAML in each section to expand the code. clusterDomain:cluster.localconfig:{}coordinating:affinity:{}autoscaling:enabled:falsemaxReplicas:3minReplicas:1targetCPU:''targetMemory:''customLivenessProbe:{}customReadinessProbe:{}customStartupProbe:{}fullnameOverride:''heapSize:128mhostAliases:[]initContainers:[]livenessProbe:enabled:truefailureThreshold:5initialDelaySeconds:90periodSeconds:10successThreshold:1timeoutSeconds:5nodeAffinityPreset:key:''type:''values:[]nodeSelector:{}podAffinityPreset:''podAnnotations:{}podAntiAffinityPreset:''podLabels:{}priorityClassName:''readinessProbe:enabled:truefailureThreshold:5initialDelaySeconds:90periodSeconds:10successThreshold:1timeoutSeconds:5replicas:1resources:limits:{}requests:cpu:25mmemory:256MischedulerName:''securityContext:enabled:truefsGroup:1001runAsUser:1001service:annotations:{}loadBalancerIP:''nodePort:''port:9200type:ClusterIPserviceAccount:create:falsename:''sidecars:[]startupProbe:enabled:falsefailureThreshold:5initialDelaySeconds:90periodSeconds:10successThreshold:1timeoutSeconds:5tolerations:[]topologySpreadConstraints:[]updateStrategy:type:RollingUpdatecurator:affinity:{}command:- curatorconfigMaps:action_file_yml:|---- actions: 1: action: delete_indices description: \"Clean up ES by deleting old indices\" options: timeout_override: continue_if_exception: False disable_action: False ignore_empty_list: True filters: - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 90 field: stats_result: epoch: exclude: Falseconfig_yml:|---- client: hosts: - {{ template \"elasticsearch.coordinating.fullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }} port: {{ .Values.coordinating.service.port }} # url_prefix: # use_ssl: True # certificate: # client_cert: # client_key: # ssl_no_validate: True # http_auth: # timeout: 30 # master_only: False # logging: # loglevel: INFO # logfile: # logformat: default # blacklist: ['elasticsearch', 'urllib3']cronjob:annotations:{}concurrencyPolicy:''failedJobsHistoryLimit:''jobRestartPolicy:Neverschedule:01***successfulJobsHistoryLimit:''dryrun:falseenabled:falseenv:{}extraInitContainers:[]extraVolumeMounts:[]extraVolumes:[]hooks:install:falseupgrade:falseimage:pullPolicy:IfNotPresentpullSecrets:[]registry:docker.iorepository:bitnami/elasticsearch-curatortag:5.8.4-debian-10-r179initContainers:[]name:curatornodeAffinityPreset:key:''type:''values:[]nodeSelector:{}podAffinityPreset:''podAnnotations:{}podAntiAffinityPreset:''podLabels:{}priorityClassName:''psp:create:falserbac:enabled:falseresources:limits:{}requests:{}schedulerName:''serviceAccount:create:truename:''sidecars:[]tolerations:[]topologySpreadConstraints:[]data:affinity:{}autoscaling:enabled:falsemaxReplicas:3minReplicas","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:1","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Kibana Next, we will deploy Kibana to view the logs in Elastic. Login to the Rancher Interface. Go to Apps and Marketplace, Charts. Select only bitnami from the chart repo options. Click on kibana menu item and click Install. Update the following and click Next: Namespace: logging-system Name: kibana-logging Customize Helm options before install: check Replace all the YAML with the below code: Update kibanaPassword: with the password your used in the Elasticsearch deployment. Update ingress.hostname: with the FQDN that you added to DNS for pass through ingress. affinity:{}configuration:server:basePath:''rewriteBasePath:falseconfigurationCM:''containerPort:5601elasticsearch:hosts:[elasticsearch-logging-master.logging-system]port:9200security:auth:enabled:trueexistingSecret:''kibanaPassword:'lockitdown'kibanaUsername:elastictls:enabled:trueexistingSecret:'elasticsearch-logging-master-crt'passwordsSecret:''truststorePassword:''usePemCerts:trueverificationMode:certificateextraConfiguration:{}extraDeploy:[]extraEnvVars:[]extraEnvVarsCM:''extraEnvVarsSecret:''extraVolumeMounts:[]extraVolumes:[]forceInitScripts:falsefullnameOverride:''global:imagePullSecrets:[]imageRegistry:''storageClass:''hostAliases:[]image:pullPolicy:IfNotPresentpullSecrets:[]registry:docker.iorepository:bitnami/kibanatag:7.15.2-debian-10-r0ingress:annotations:{}apiVersion:''enabled:trueextraHosts:[]extraPaths:[]extraTls:[]hostname:kibana.apps.dsodev.netpath:/pathType:ImplementationSpecificsecrets:[]tls:trueinitContainers:[]initScriptsCM:''initScriptsSecret:''kubeVersion:''livenessProbe:enabled:truefailureThreshold:6initialDelaySeconds:120periodSeconds:10successThreshold:1timeoutSeconds:5metrics:enabled:falseservice:annotations:prometheus.io/path:_prometheus/metricsprometheus.io/port:'80'prometheus.io/scrape:'true'serviceMonitor:enabled:falseinterval:''namespace:''scrapeTimeout:''selector:{}nameOverride:''nodeAffinityPreset:key:''type:''values:[]nodeSelector:{}persistence:accessMode:ReadWriteOnceenabled:trueexistingClaim:''size:10GistorageClass:''plugins:[]podAffinityPreset:''podAnnotations:{}podAntiAffinityPreset:softpodLabels:{}readinessProbe:enabled:truefailureThreshold:6initialDelaySeconds:30periodSeconds:10successThreshold:1timeoutSeconds:5replicaCount:1resources:limits:{}requests:{}savedObjects:configmap:''urls:[]schedulerName:''securityContext:enabled:truefsGroup:1001runAsNonRoot:truerunAsUser:1001service:annotations:{}externalTrafficPolicy:ClusterextraPorts:[]labels:{}loadBalancerIP:''nodePort:''port:5601type:ClusterIPserviceAccount:annotations:{}create:truename:''sidecars:[]tls:autoGenerated:falseenabled:falseexistingSecret:''keyPassword:''keystorePassword:''passwordsSecret:''usePemCerts:falsetolerations:[]updateStrategy:type:RollingUpdatevolumePermissions:enabled:falseimage:pullPolicy:IfNotPresentpullSecrets:[]registry:docker.iorepository:bitnami/bitnami-shelltag:10-debian-10-r248resources:{}Click Next and click Install. Verify that everthing is up by running the following command: kubectl get all -n logging-system ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:2","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Installing Fluent-bit Next we will install Fluent-bit to actually ship the logs out of Kubernetes and into Elasticsearch. Login to the Rancher Interface. Go to Apps and Marketplace, Repositories. Click Create and add the following then click Create again: Name: fluent Target: Git Git Repo URL: https://github.com/fluent/helm-charts Git Branch: main Click on fluent-bit menu item and click Install. Update the following and click Next: Namespace: logging-system Name: fluentbit-logging Customize Helm options before install: check Replace all the YAML with the below code. Update HTTP_Passwd with the password your used in the Elasticsearch deployment. # Default values for fluent-bit.# kind -- DaemonSet or Deploymentkind:DaemonSet# replicaCount -- Only applicable if kind=DeploymentreplicaCount:1image:repository:fluent/fluent-bit# Overrides the image tag whose default is {{ .Chart.AppVersion }}tag:\"\"pullPolicy:AlwaystestFramework:image:repository:busyboxpullPolicy:Alwaystag:latestimagePullSecrets:[]nameOverride:\"\"fullnameOverride:\"\"serviceAccount:create:trueannotations:{}name:rbac:create:truenodeAccess:falsepodSecurityPolicy:create:falseannotations:{}podSecurityContext:{}# fsGroup: 2000hostNetwork:falsednsPolicy:ClusterFirstdnsConfig:{}# nameservers:# - 1.2.3.4# searches:# - ns1.svc.cluster-domain.example# - my.dns.search.suffix# options:# - name: ndots# value: \"2\"# - name: edns0hostAliases:[]# - ip: \"1.2.3.4\"# hostnames:# - \"foo.local\"# - \"bar.local\"securityContext:{}# capabilities:# drop:# - ALL# readOnlyRootFilesystem: true# runAsNonRoot: true# runAsUser: 1000service:type:ClusterIPport:2020labels:{}# nodePort: 30020annotations:{}# prometheus.io/path: \"/api/v1/metrics/prometheus\"# prometheus.io/port: \"2020\"# prometheus.io/scrape: \"true\"serviceMonitor:enabled:false# namespace: monitoring# interval: 10s# scrapeTimeout: 10s# jobLabel: fluentbit# selector:# prometheus: my-prometheus# ## metric relabel configs to apply to samples before ingestion.# ### metricRelabelings:# - sourceLabels: [__meta_kubernetes_service_label_cluster]# targetLabel: cluster# regex: (.*)# replacement: ${1}# action: replace# ## relabel configs to apply to samples after ingestion.# ### relabelings:# - sourceLabels: [__meta_kubernetes_pod_node_name]# separator: ;# regex: ^(.*)$# targetLabel: nodename# replacement: $1# action: replaceprometheusRule:enabled:false# namespace: \"\"# additionnalLabels: {}# rules:# - alert: NoOutputBytesProcessed# expr: rate(fluentbit_output_proc_bytes_total[5m]) == 0# annotations:# message: |# Fluent Bit instance {{ $labels.instance }}'s output plugin {{ $labels.name }} has not processed any# bytes for at least 15 minutes.# summary: No Output Bytes Processed# for: 15m# labels:# severity: criticaldashboards:enabled:falselabelKey:grafana_dashboardannotations:{}livenessProbe:httpGet:path:/port:httpreadinessProbe:httpGet:path:/api/v1/healthport:httpresources:{}# limits:# cpu: 100m# memory: 128Mi# requests:# cpu: 100m# memory: 128MinodeSelector:{}tolerations:[]affinity:{}labels:{}annotations:{}podAnnotations:{}podLabels:{}priorityClassName:\"\"env:[]envFrom:[]extraContainers:[]# - name: do-something# image: busybox# command: ['do', 'something']extraPorts:[]# - port: 5170# containerPort: 5170# protocol: TCP# name: tcp# nodePort: 30517extraVolumes:[]extraVolumeMounts:[]updateStrategy:{}# type: RollingUpdate# rollingUpdate:# maxUnavailable: 1# Make use of a pre-defined configmap instead of the one templated hereexistingConfigMap:\"\"networkPolicy:enabled:false# ingress:# from: []luaScripts:{}## https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/configuration-fileconfig:service:|[SERVICE] Daemon Off Flush 1 Log_Level {{ .Values.logLevel }} Parsers_File parsers.conf Parsers_File custom_parsers.conf HTTP_Server On HTTP_Listen 0.0.0.0 HTTP_Port {{ .Values.service.port }} Health_Check On## https://docs.fluentbit.io/manual/pipeline/inputsinputs:|[INPUT] Name tail Path /var/log/containers/*.log multiline.parser docker, cri Tag kube.* Mem_","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:3","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Accessing Logs With Kibana Lastly, we need to login to Kibana, add the search index pattern, and start viewing the logs. In a browser, navigate to the FQDN you defined for your Kibana instance. eg: https://kibana.apps.dsodev.net. Login using: Username: elastic Password: the password your used in the Elasticsearch deployment. Click the burger menu in the upper left and select Stack Management at the bottom. Under Kibana, select Index Patterns. Click Create Index Pattern. Enter the following details and click Create Index Pattern: Name: logstash* Timestamp: @timestamp Click the burger menu in the upper left and select Discover. You should see logs from all pods in your cluster flowing into this interface. Kibana UI\" Kibana UI ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:8:4","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Closing Based on the instructions in the “Single Node RKE2” blogs, you should now have a solid foundation cluster for running, monitoring, and logging workloads on Kubernetes. Deploying RKE2 or K3s as a highly available, multi-node, production-ready setup involves quite a few more steps. But this guide hopefully gives you the starting point and confidence to go start building that out for yourself or your organization. If you have any questions or would like AlphaBravo’s assistance in building production-grade Kubernetes clusters, please reach out to us at info@alphabravo.io. Thanks for reading! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us ","date":"2021-12-06","objectID":"/posts/2021/single-node-rke2-pt2/:9:0","tags":["rancher","kubernetes","rke2","helm","hetzner","longhorn","elasticsearch","grafana","kibana","prometheus"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 2)","uri":"/posts/2021/single-node-rke2-pt2/"},{"categories":null,"content":"Instructions on deploying K3s, Helm, cert-manager and Rancher on a single node Hetzner server.","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"The K3s Cluster No, your eyes do not deceive you. This is a very similarly named blog to our Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1). Some folks don’t want to deploy RKE2, and we don’t want you to feel left out. In this blog we are going to deploy K3s in place of RKE2. If you want to see why we chose Hetzner for this specific deployment, go check out the previous blog. ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:1:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Why K3s K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances. There are a few key differences (as of writing) between K3s and RKE2. K3s supports ARM processors (like Raspberry Pis). K3s is not as focused on FIPS, CIS and other compliance standards as RKE2 K3s deploys with LocalStorage enabled for Persistent Volumes K3s deploys with Traefik ingress controller instead of NGINX ingress controller ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:2:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Deploying a VM IMPORTANT Make sure you keep your SSH keys safe, lock the system down with appropriate firewall rules, and keep the OS updated. So, let’s get started. If you don’t already have a Hetzner account, go sign up here: Hetzner Signup* Once you are signed up: Log in to Hetzner Cloud Click on Default project, or create a new project if you like Click Add Server and select the following options Location: Ashburn, VA (or your desired region) Image: Ubuntu 20.04 Type: Standard and CPX51 Volume: None Network: Create a new one if you like Firewalls: Create a new one that allows all access from your IP and only 80/443 from All IPs SSH Key: Create a new SSH key to use or use an existing one Name: K3s-Dev Click Create \u0026 Buy Now This will provision this new server and provide you and external IP Address that you can use to connect to the node. ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:3:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Install K3S Kubernetes Once the node is provisioned, get the IP and login to the cluster using the SSH key you configured. In a terminal, run ssh -i /path/to/key/id_rsa root@YOURSERVERIP Update the OS apt update \u0026\u0026 apt upgrade -y Install K3s: curl -sfL https://get.k3s.io | sh - Export the kubeconfig file. We can use this file to connect from our local machine using the kubectl command line or Lens mkdir ~/.kube cp /etc/rancher/k3s/k3s.yaml ~/.kube/config Run kubectl get nodes to verify that the RKE2 node is up and running root@k3s-dev:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-dev Ready control-plane,master 4m34s v1.21.5+k3s2 You now have a single node K3s cluster up and running. ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:4:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Configure DNS (Optional) If you would like the cluster to have a DNS name and be able to pull valid Let’s Encrypt keys, you will need to point some DNS names to your cluster. Add the following to you public DNS records for your desired domain. (We use Cloudflare at AB. Reach out if you have questions.) rancher.YOURDOMAIN.tld -\u003e IP of your Hetzner Server (This will let you reach the Rancher interface we configure in the next steps) *.apps.YOURDOMAIN.tld -\u003e IP of your Hetzner Server (This will let you reach apps running in Kubernetes via NGINX Ingress) ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:5:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Install Helm We will use Helm on the Hetzner machine to deploy Rancher (and other applications if you want). Run the following commands to install Helm: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:6:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Deploy Rancher We can now deploy Rancher 2.6 on the cluster. We will use Rancher as the Management UI for the cluster. Official documentation: https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/ Add the Rancher Stable Helm Repo helm repo add rancher-stable https://releases.rancher.com/server-charts/stable Create a namespace for Rancher kubectl create namespace cattle-system Install cert-manager for management of SSL certificates in cluster # Install cert-manager CRDs kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.5.1 Install Rancher using Helm. This assumes you have configured DNS to get a valid certificate. helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.YOURDOMAIN.tld \\ --set bootstrapPassword=SUPERSECRETADMINPASSWORD Verify rancher deployed successfully kubectl -n cattle-system rollout status deploy/rancher We need to allow Traefik to communicate on 80/433 on our host. To do that, create a file named traefik.yml and add the following. apiVersion:v1kind:Servicemetadata:name:traefiknamespace:kube-systemspec:type:LoadBalancerApply the the file using kubectl kubectl apply -f traefik.yml Once Rancher has been fully deployed, visit https://rancher.YOURDOMAIN.tld in your browser to login to your Rancher UI and interact with your cluster. Now you should have a fully functioning K3s Single Node Cluster with Rancher Management Interface installed. Rancher UI on K3s\" Rancher UI on K3s ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:7:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Closing Based on the instructions above, you should now have a powerful K3s Kubernetes cluster you can use for testing and development work. In the next part of this series, we will go back to RKE2 and install Monitoring and Longhorn for persistent volumes in the cluster. If you have any questions, please reach out to us at devops@alphabravo.io. Thanks for reading! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us Note: Some links included here are affiliate links. If you use the link to sign up, AB may get a small referral credit on our account. ","date":"2021-11-30","objectID":"/posts/2021/single-node-k3s/:8:0","tags":["rancher","kubernetes","k3s","helm","hetzner"],"title":"Powerful Single Node K3s on Hetzner for CHEAP!","uri":"/posts/2021/single-node-k3s/"},{"categories":null,"content":"Instructions on deploying RKE2, Helm, cert-manager and Rancher on a single node Hetzner server.","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"The RKE2 Cluster In this blog, we will explain how to deploy a powerful, single node RKE2 cluster on Hetzner. It will provide the following resources. Single Node RKE2 Kubernetes Cluster NGINX Ingress into your cluster Rancher Multi Cluster Manager Web UI to manage your cluster ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:1:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Why Hetzner? Want an easy, powerful, single node, publically accessible Kubernetes cluster? Well, do we have a deal for you! (said in my best Billy Mays voice.) For this, you can use any cloud service. We are recommending Hetzner because they are a reliable and reputable company with many of years of experience operating in the EU and they just opened a data center in US East (Northern Virgina). For the below specs, just look at the price difference. Hetzner: 16 vCPU (AMD) 32GB RAM 360GB SSD 20TB Traffic Hetnzer Price: 49.90/mo Euro ($57.00/mo USD) DigitalOcean: 16 vCPU (Intel) 32GB RAM 50GB SSD 7TB Traffic DigitalOcean Price: $320/mo USD That is a SIGNIFICANT difference in price vs specs. You could run a 5 node cluster in Hetzner for the same price as a single Digital Ocean node. This makes Hetzner ideal for development environments. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:2:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Why RKE2 We will be using RKE2 by Rancher. RKE2 is like K3s in that it is a CNCF certified, single binary Kubernetes installation. RKE2, also known as RKE Government, is Rancher’s next-generation Kubernetes distribution. It is a fully conformant Kubernetes distribution that focuses on security and compliance within the U.S. Federal Government sector. To meet these goals, RKE2 does the following: Provides defaults and configuration options that allow clusters to pass the CIS Kubernetes Benchmark v1.5 or v1.6with minimal operator intervention Enables FIPS 140-2 compliance Regularly scans components for CVEs using trivy in our build pipeline ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:3:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"How is this different from RKE or K3s? RKE2 combines the best-of-both-worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s. From K3s, it inherits the usability, ease-of-operations, and deployment model. From RKE1, it inherits close alignment with upstream Kubernetes. In places K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream. Importantly, RKE2 does not rely on Docker as RKE1 does. RKE1 leveraged Docker for deploying and managing the control plane components as well as the container runtime for Kubernetes. RKE2 launches control plane components as static pods, managed by the kubelet. The embedded container runtime is containerd. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:3:1","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Deploying a VM IMPORTANT Make sure you keep your SSH keys safe, lock the system down with appropriate firewall rules, and keep the OS updated. So, let’s get started. If you don’t already have a Hetzner account, go sign up here: Hetzner Signup* Once you are signed up: Log in to Hetzner Cloud Click on Default project, or create a new project if you like Click Add Server and select the following options Location: Ashburn, VA (or your desired region) Image: Ubuntu 20.04 Type: Standard and CPX51 Volume: None Network: Create a new one if you like Firewalls: Create a new one that allows all access from your IP and only 80/443 from All IPs SSH Key: Create a new SSH key to use or use an existing one Name: RKE2-Dev Click Create \u0026 Buy Now This will provision this new server and provide you and external IP Address that you can use to connect to the node. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:4:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Install RKE2 Kubernetes Once the node is provisioned, get the IP and login to the cluster using the SSH key you configured. In a terminal, run ssh -i /path/to/key/id_rsa root@YOURSERVERIP Update the OS apt update \u0026\u0026 apt upgrade -y Install RKE2: curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL=v1.21.6+rke2r1 sh - systemctl enable --now rke2-server.service export PATH=/var/lib/rancher/rke2/bin:$PATH Export the kubeconfig file. We can use this file to connect from our local machine using the kubectl command line or Lens mkdir ~/.kube cp /etc/rancher/rke2/rke2.yaml ~/.kube/config Run kubectl get nodes to verify that the RKE2 node is up and running root@rke2-test:~# kubectl get nodes NAME STATUS ROLES AGE VERSION rke2-dev Ready control-plane,etcd,master 12d v1.21.6+rke2r1 You now have a single node RKE2 cluster up and running. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:5:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Configure DNS (Optional) If you would like the cluster to have a DNS name and be able to pull valid Let’s Encrypt keys, you will need to point some DNS names to your cluster. Add the following to you public DNS records for your desired domain. (We use Cloudflare at AB. Reach out if you have questions.) rancher.YOURDOMAIN.tld -\u003e IP of your Hetzner Server (This will let you reach the Rancher interface we configure in the next steps) *.apps.YOURDOMAIN.tld -\u003e IP of your Hetzner Server (This will let you reach apps running in Kubernetes via NGINX Ingress) ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:6:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Install Helm We will use Helm on the Hetzner machine to deploy Rancher (and other applications if you want). Run the following commands to install Helm: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:7:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Deploy Rancher We can now deploy Rancher 2.6 on the cluster. We will use Rancher as the Management UI for the cluster. Official documentation: https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/ Add the Rancher Stable Helm Repo helm repo add rancher-stable https://releases.rancher.com/server-charts/stable Create a namespace for Rancher kubectl create namespace cattle-system Install cert-manager for management of SSL certificates in cluster # Install cert-manager CRDs kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.5.1 Install Rancher using Helm. This assumes you have configured DNS to get a valid certificate. helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.YOURDOMAIN.tld \\ --set bootstrapPassword=SUPERSECRETADMINPASSWORD Verify rancher deployed successfully kubectl -n cattle-system rollout status deploy/rancher Once Rancher has been fully deployed, visit https://rancher.YOURDOMAIN.tld in your browser to login to your Rancher UI and interact with your cluster. Now you should have a fully functioning RKE2 Single Node Cluster with Rancher Management Interface installed. Rancher UI on RKE2\" Rancher UI on RKE2 ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:8:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Closing Based on the instructions above, you should now have a powerful RKE2 Kubernetes cluster you can use for testing and development work. In Part 2 of this blog series, we will install Monitoring and Longhorn for persistent volumes in the cluster. If you have any questions, please reach out to us at devops@alphabravo.io. Thanks for reading! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us Note: Some links included here are affiliate links. If you use the link to sign up, AB may get a small referral credit on our account. ","date":"2021-11-29","objectID":"/posts/2021/single-node-rke2-pt1/:9:0","tags":["rancher","kubernetes","rke2","helm","hetzner"],"title":"Powerful Single Node RKE2 on Hetzner for CHEAP! (Part 1)","uri":"/posts/2021/single-node-rke2-pt1/"},{"categories":null,"content":"Why AlphaBravo Chose Nifi Over Databrew Recently, the AlphaBravo engineering team began working on a new product we’re calling ABScan. Our goal is to provide a platform which uses multiple scanning utilities via a single interface. Early on in our development process, we realized that we would need to take multiple JSON outputs, convert them into CSV and SQL, and merge the results. This would require the use of an ETL to automate the transformation of the data generated so we could create CSV files for reports and SQL data for long term storage. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:1:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What is an ETL? ETL stands for “Extract, Transform, and Load”. It is a term used when dealing with data, data warehousing, and data analytics. It is a means of bringing in data from multiple sources into a centralized system, such as a database. It works by performing the following work: Extract: Collect data from an originating source, such as one or more scanning applications. Transform: Manipulate the data by merging, deduplication, formatting. Load: Using the transformed data sources, load the information into a file or database. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:2:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What is AWS Gule Databrew? When we first began working on the project, we explored ETL platforms which could get us to market as quickly as possible. Given our regular use of the Amazon Web Services (AWS) platform, we reviewed the options available and found Glue Databrew. Glue Databrew is a visual data preparation tool which allows for easy cleaning and normalization of data in preparation for use with analytics and machine learning. The engineering team discovered we could easily upload a dataset, create repeatable recipes for our ETL purposes, and save our results to a database or S3 bucket. The stand-out feature for Databrew is it’s easy to use interface and time-to-market, in addition to easily integrating into AWS platforms and services. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:3:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What is Apache Nifi? During our discovery phase, we also researched Apache’s Nifi. Much like AWS Glue Databrew, Apache Nifi provides a visual data preparation tool to clean and normalize data. Using a series of processors, Nifi can extract data from multiple sources, create multiple workflows for ETL purposes, and save the results to a database or S3 bucket. The stand-out feature for Nifi, as compared to AWS, is its ability to allow AlphaBravo to be platform agnostic, whereas Databrew is hard wired into the AWS ecosystem. Nifi also scales exceptionally well and is incredibly fast. What Databrew Does Well During our testing with Databrew, we discovered that the interface is incredibly simple to understand and use. The engineering team were able to create multiple workflow samples within a day, and by the end of day two, we were able to ingest multiple data samples, perform ETL work, merge all of the results into a single dataset, and load the results into an S3 bucket or database. Given that Databrew is a part of the AWS ecosystem, it’s trivial to automate against the platform using multiple toolsets. The AWS CLI works exceptionally well, as does the SDK toolsets, such as the BOTO SDK for Python. Using these tools, we were able to get Databrew completely integrated into our automation pipeline within a few days. Using the UI, the team was able to build easy to use recipes, which we could apply to our sample datasets and return the same result time and time again, as long as the dataset matched the expectations of the recipe. Once we had agreed upon the recipes for our sample datasets, we were able to use these recipes to create jobs with associated datasets to perform the ETL workload. Overall, from start to finish the process rarely took over a few minutes to complete. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:4:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What Databrew Lacks For all of its features, Databrew does have a few weaknesses which made us decide to move off the platform towards something else. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Limits One of the biggest issues we encountered while using Glue Databrew was the inherent limits of the platform. Out of the box, only 10 jobs could be running at any given time. While we could reach out to AWS to increase this limit (and we did), each time we hit a hard limit we would have to reach out to support to resolve the issue. Support would be able to increase the limit, given a good enough reason was provided to do so. Our concern here was we might need to increase the limit significantly at some point but AWS would be slow to handle the increase, or worse, be unwilling to do so. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:1","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Timeouts During our testing process, we encountered a significant number of timeout issues. We realized this was due to the total number of calls being made against the AWS API, and was the result of the engineering team working in parallel and making a non-trivial number of calls to the platform at any given time. We reached out to support to see what we could do to solve these problems, however we were informed that the only way to solve the issue would be to build in logic to throttle API calls over time to prevent the timeout from occurring. This would be a difficult problem to solve over time as we expected to continue to increase API calls as the platform grew, and we were experiencing severe timeouts even during a simple POC. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:2","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Recipe Issues We discovered after running a significant number of datasets that we might have extra information in one random dataset as compared to the rest. Unfortunately, if a recipe is not expecting the extra data, or if data is missing for any reason, the job will fail as the recipe does not understand how to continue. Unfortunately, there is no current method to build in conditions in a given recipe, so the entire job would fail. Further, this would require the team to test a significant number of data samples, determine how any given data sample might fail, and build a specific recipe for the use case. We would then have to build logic to determine what recipes a given dataset would require and build the job around those requirements. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:3","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Job Spin-up Time Glue Databrew is a “just-in-time” platform, meaning the jobs engine stays offline until a call is made to spin up the underlying servers to run a given job. As a result, it can take some time, especially for larger datasets, to complete as each job submitted must wait for the platform to come online. Our average jobs would take a few minutes each to run. Given the limit issue mentioned earlier, this would also become a blocker for other jobs in queue and quickly create a backlog from our automation system. The more people submit data, the longer the last person who created a submission must wait. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:4","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Work Required for Automation Integration To automate against Glue Databrew, a series of actions must be written to perform work against the service. A dataset must be created, referencing a data source such as a database or S3 bucket. A recipe which will work against the dataset must exist or be uploaded. Given that someone can inadvertently delete a recipe from the UI as there’s no way to lock them, it makes sense to upload the recipe as each dataset is created. A means to validate the dataset and recipe have uploaded successfully before a job is submitted. A job submission, which references the dataset and recipe. A method to validate the total number of jobs currently in progress to avoid hitting limits and failure. A timeout function to prevent too many calls from being made against the API, and to back off on API calls over time to prevent the timeout from occurring. A method to verify when a job is complete to determine if the ETL process has completed and the data has output successfully. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:5:5","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What Nifi Does Well As with Databrew, the Apache Nifi platform has a very simple and easy to use interface. Out of the box, it provides a significant number of processors which are used to handle many kinds of data workflows. Nifi easily integrates with many cloud platforms and service providers, providing simple methods to extract data. It can query and load many different database platforms, message queues, email, APIs, streaming services, logs, ftp, etc. And if you cannot find a processor to fit your needs, you can create your own and import it into the service. Once data has been pulled into the platform, Nifi can easily transform the data and move it along in different workflows. For example, I can pull a JSON document from a message queue, extract the information contained in the file to set one or more attributes, create a flowfile using the attributes, deduplicate and sort the information in the flowfile, merge the contents into a new CSV file, and store the newly created CSV file into an S3 bucket. Did I mention Nifi is fast? It can do all of the work mentioned above in less than a second. In testing, the AphaBravo engineering team has thrown significant amounts of data at Nifi and has been able to retrieve data within a second or two. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:6:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What Nifi Lacks While Nifi is powerful, it does have a few significant limitations as compared to Glue Databrew which had to be considered: ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Self-Hosted Apache Nifi currently, as of the time of this writing, does not have a hosted service available. As such, we had to spin up our own environment. This has a lot of added complexity as compared to Glue Databrew, we have to build out a platform which is powerful enough to process our data without breaking the bank. This also means the engineering team must work on regular updates for the product, as well as manage configurations and securing secrets, which adds additional overhead for the use of the product. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:1","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Scalability Given the Nifi platform is self-hosted, the AlphaBravo team must also take into account the need to scale the platform on-demand. While Glue Databrew can be easily scaled by submitting a support request to increase limits, scale for Nifi must be planned, and preferably, managed automatically. When load on the platform is low, we will not need multiple Nifi nodes. However, if and when we receive regular or bursting traffic, there is a possibility of bottlenecks if we don’t have a means to scale, which can cause ever increasing wait times for users of the platform. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:2","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Always On Unlike Glue Databrew Jobs, Apache Nifi is always on and listening for input. This is, in part, what makes it so fast to process data. However, the downside is the requirements for infrastructure which is always online and must be highly available. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:3","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Lack of Informative Documentation While the Nifi platform has a significant amount of documentation, it’s not very informative. There have been several instances during the POC where a processor was introduced and experienced failures because the documentation was not very clear to the expectations of work. As an example, database integration can be tricky to set up. A user must first find the related processor, for example, “ExecuteSQL”. In order to use this processor, a database connection pooling service must be assigned. If this controller service does not yet exist, it must be created and configured. When setting up the controller service for the database connection pool, you must know the following: The database connection URL: Not only do you need the URL itself, but you must use the exact syntax required by your DBMS. Example: jdbc:mysql://mydatabase:3306 The database driver class name: The name of the driver class to use for connectivity. This is usually provided via the database documentation, and isn’t referenced in the Nifi documentation. Example: com.mysql.jdbc.Driver The location of the database driver on the host node: Generally, the driver is not installed with Nifi. You must download the driver, upload it to the hosted node(s), extract the driver, and store it in a directory which Nifi can access. Example: /opt/nifi/nifi-current/drivers/mysql ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:7:4","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"So Why Did We Choose Nifi over Databrew? After a significant amount of testing with these two platforms, the AphaBravo engineering team chose Nifi for the following reasons, in no particular order: ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:0","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Popularity Nifi is a very, very popular toolset in the ETL community. It is used by many large organizations, as well as government entities. Nifi itself originated from the NSA, based on the NiagraFiles software which was open sourced by the NSA in 2014. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:1","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Age of the Nifi Project Nifi was originally open sourced in 2014, and has an active community. Glue Databrew was released at the end of 2020, but the Glue platform on which it is based was originally released in 2017. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:2","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Platform Agnostic While Glue Databrew is tightly integrated with AWS, Nifi works with all major cloud providers and a host of 3rd party tool sets. This allows the engineering team to host Nifi in many different environments and allows movement from platform to platform without breaking functionality. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:3","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Ability to Scale Nifi scale is only limited to the AlphaBravo team’s capabilities, and is not based on arbitrary limits set by a cloud provider. We could, in theory, scale the platform indefinitely should the need to do so arise. ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:4","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"Speed Nifi is fast. So fast, in fact, we had to build in timing mechanisms in certain workflows during our POC because the data was collecting so quickly that multiple files were being created inadvertently during data merge processes. We introduced bin timeouts to solve this problem. As a result, the slowest workflow takes approximately 10 seconds to complete end to end. Ed Engelking - AlphaBravo Principal Engineer Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us ","date":"2021-11-22","objectID":"/posts/2021/why-we-chose-nifi-over-databrew/:8:5","tags":["apache nifi","aws databrew","etl"],"title":"Why We Chose Nifi Over Databrew","uri":"/posts/2021/why-we-chose-nifi-over-databrew/"},{"categories":null,"content":"What is Rancher Multi-Cluster Management If you have ventured into container management and Kubernetes for your organization (or even for yourself), you have quickly realized that you need a robust way to manage more than one cluster easily. In fact, you probably want an easier way to manage a single cluster. The official Kubernetes dashboard is… sub-optimal. There are a few other open-source options out there, but they lack in many areas. So what are we supposed to do? # Rancher has entered the chat. This is where the Rancher UI, or Rancher Multi-Cluster Manager (Rancher MCM) comes into play. As engineers responsible for a plethora of technologies, even just drilling down into Kubernetes space we usually have: A local Kubernetes cluster (k3d, k3s on a Raspberry Pi, or similar) 1-2 Dev clusters 1-N Test, QA, etc clusters 1-N Production clusters On each of those clusters we need: Monitoring \u0026 Alerting Logging Backups Direct access to Pod console and logs Simplified deployment capabilities (Helm, etc) into each Quick view into the health of the cluster Much more This presents a problem because our time is already fragmented enough. We really need a unified interface that not only lowers the barrier to entry for less experienced Kubernetes users but also simplifies cluster and app administration for our fragmented attention. ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:1:0","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"What Can Rancher MCM Do? Rancher MCM has numerous capabilities that will make the developer, infrastructure, security and cluster-administrators life much easier. Some of the key features of Rancher MCM are: Certified Distritbution Support: Rancher supports any certified Kubernetes distribution. For on-premises workloads, we offer the RKE. For the public cloud, we support all the major distributions, including EKS, AKS, and GKE. For edge, branch and desktop workloads we offer K3s, a certified lightweight distribution of Kubernetes. Simplified Cluster Operations: Rancher provides simple, consistent cluster operations including provisioning, version management, visibility and diagnostics, monitoring and alerting, and centralized audit. Security, Policy and User Management: Rancher lets you automate processes and applies a consistent set of user access and security policies for all your clusters, no matter where they’re running. Shared Tools and Services: Rancher provides a rich catalog of services for building, deploying and scaling containerized applications, including app packaging, CI/CD, logging, monitoring and service mesh. ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:2:0","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"Deploying Rancher MCM There are 2 primary (well-documented) ways that we generally deploy Rancher MCM. ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:3:0","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"Single Host on Docker (Non HA) Docker Install Docker Advanced Options There are many scenarios where we need to stand up a temporary or demo instance of Rancher. While the production, highly available method is to deploy the Helm chart onto a HA Kubernetes cluster, the simplest way to get started is to deploy Rancher as a Docker container. Before you start, make sure you have a modern Linux distro running with a recent version of Docker running. The simple command which includes local persistent storage is: docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ -v /opt/rancher:/var/lib/rancher \\ --privileged \\ rancher/rancher:latest If you are running this on a public server, you can also automatically deploy this and get a valid SSL certificate from Let’s Encrypt using the following command. Make sure that the following is configured: Open port 80 from the internet to your server Configure the \u003cYOUR.DNS.NAME\u003e that you enter below to point to the public IP of your server docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ -v /opt/rancher:/var/lib/rancher \\ --privileged \\ rancher/rancher:latest --acme-domain \u003cYOUR.DNS.NAME\u003e Check out the documentation for additional advanced deployment options for the Rancher Single Node Docker Install Fun Fact: The Docker Install actually deploys a K3d Single Node Cluster under to the hood to run the Rancher MCM UI on top of ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:3:1","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"HA Deployment of Kubernetes Rancher on K8s Docker Advanced Options Assuming you have a HA (at least 3 master node) Kubernetes cluster up and running already, you can deploy Rancher on top of that to provide you with a HA deployment of you Rancher MCM. In this instance, we will use the Rancher self generated SSL cert, but you can refer to the docs for apply your own SSL certificates. Before you get started you will need: A HA Kubernetes Cluster. Any one will do. Helm installed locally First, you need to install cert-manager in your cluster: # If you have installed the CRDs manually instead of with the `--set installCRDs=true` option added to your Helm install command, you should upgrade your CRD resources before upgrading the Helm chart: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.5.1 We can verify cert-manager is installed by running: kubectl get pods --namespace cert-manager Next, we can install Rancher: # Add the Rancher Helm repository helm repo add rancher-stable https://releases.rancher.com/server-charts/stable # Create the Rancher Namespace kubectl create namespace cattle-system # Install the Rancher Helm chart helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.my.org \\ --set bootstrapPassword=admin We can verify our rancher deployment using the following command: kubectl -n cattle-system rollout status deploy/rancher Now you should be able to visit https://rancher.my.org and access the cluster. You can now begin to import additional clusters, select existing clusters to explore and deploy new resources. Here are some videos to get you started with the Rancher 2.6 Interface: Rancher Online Meetup - September 2021 - Introducing SUSE Rancher 2.6 What’s New in Rancher 2.6? Rancher 2.6 UI Walkthrough and Q\u0026A ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:3:2","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"Rancher MCM - Final Words Hopefully, this post provided you with some insight into how valuable the Rancher MCM UI can be to your organization and helped you get started deploying it into your environment. Please reach out to us if you have any questions regarding installing Rancher or if you need any help running your DevSecOps systems. Thanks for reading! The AB Engineering Team Website: https://alphabravo.io Contact Us: https://alphabravo.io/contact-us ","date":"2021-11-17","objectID":"/posts/2021/rancher-multi-cluster-management-ui/:3:3","tags":["rancher","kubernetes","rancher mcm"],"title":"Rancher Multi-Cluster Management UI","uri":"/posts/2021/rancher-multi-cluster-management-ui/"},{"categories":null,"content":"Some insight into why AlphaBravo uses Rancher technologies to accelerate DevSecOps initiatives internally and for our customers.","date":"2021-11-15","objectID":"/posts/2021/why-we-use-rancher/","tags":["rancher","kubernetes","rke2","k3s"],"title":"Why We Use Rancher","uri":"/posts/2021/why-we-use-rancher/"},{"categories":null,"content":"Why We Use Rancher Ok, let’s get this out of the way. Yes, we are a Rancher Gold Partner and yes, we provide Rancher Training. We are not those things because we are corpo shills. We partnered with Rancher because we use their products anyway. We use them in production (RKE2 and K3s). We use them on our Raspberry Pi clusters (K3s). We use them in Docker (K3d) And we manage it all with the Rancher UI. So what are some of the reasons why we use Rancher? The products are open source. At AlphaBravo we love Open Source technology and companies that find a way to open source their technology and still provide immense value beyond that open source project. Secure (or at least securable with minimal configuration tweaks). There are varying “out of the box” security settings across the product line, but Rancher does a great job of focusing on differentiating in each area (K3s vs RKE2 for example). Simple to deploy. This is a big one. As engineers who need to do our full-time jobs while also keeping track of the latest technology, it is important to be able to quickly deploy and assess new technologies is critical. With Rancher software and some basic Linux knowledge and some Linux systems around, you can deploy many of their products including a functioning Kubernetes cluster with a SINGLE CLI COMMAND. Let that sink in. Easy to use. When it is 2 AM and you are troubleshooting a production micro-service issue, you definitely need this as a feature. (Shout out to my Pager Duty crew.) I am obviously kidding. We should all be striving to expand our technical capabilities, play well with others, and operate as one big Dev and Ops family. Breaking down the silos and sharing our knowledge is what makes us stronger as a team. Rancher certainly assists in that mission. I mean, have you SEEN the sheer number of open-source products they have? Rancher UI - The Enterprise Kubernetes Management Platform. Rancher is a complete platform for managing Kubernetes clusters wherever you deploy them. RKE - RKE is a CNCF-certified Kubernetes distribution that runs entirely within Docker containers. It solves the common frustration of installation complexity with Kubernetes by removing most host dependencies and presenting a stable path for deployment, upgrades, and rollbacks. RKE2 - RKE2, also known as RKE Government, is Rancher’s next-generation Kubernetes distribution. It is a fully conformant Kubernetes distribution that focuses on security and compliance within the U.S. Federal Government sector. K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances. K3os - k3OS is purpose-built to simplify Kubernetes operations in low-resource computing environments. Installs fast. Boots faster. Managed through Kubernetes. k3d - k3d is a lightweight wrapper to run k3s (Rancher Lab’s minimal Kubernetes distribution) in docker. k3d makes it very easy to create single- and multi-node k3s clusters in docker, e.g. for local development on Kubernetes. Longhorn - Longhorn is a lightweight, reliable, and easy-to-use distributed block storage system for Kubernetes. Longhorn is free, open-source software. Originally developed by Rancher Labs, it is now being developed as an incubating project of the Cloud Native Computing Foundation. Fleet - Fleet is GitOps at scale. Fleet is designed to manage up to a million clusters. It’s also lightweight enough that it works great for a single cluster too, but it really shines when you get to a large scale. By large scale, we mean either a lot of clusters, a lot of deployments, or a lot of teams in a single organization. Harvester - Harvester is a modern Hyperconverged infrastructure (HCI) solution built for bare metal servers using enterprise-grade open source technologies including Kubernetes, Kubevirt, and Longhorn. Designed for users looking for a cloud-native HCI solution, Harvester is a flexible and affordable offerin","date":"2021-11-15","objectID":"/posts/2021/why-we-use-rancher/:0:0","tags":["rancher","kubernetes","rke2","k3s"],"title":"Why We Use Rancher","uri":"/posts/2021/why-we-use-rancher/"},{"categories":null,"content":"Welcome To The Blog We are excited to have you here. As technology enthusiasts with a broad range of experiences, we hope to convey some tales, some tech, and some thoughts around the current state of DevSecOps. Our focus is mainly on building robust platforms using open-source software. But don’t be confused if we talk about non-open source software too. We are a small team and we understand very well that sometimes you need a product or tool that “just works”. While we provide technology services in staff augmentation, deployment acceleration, and training, we also are building some cool internal tools that we hope to release to the community soon. Thanks for checking us out and we look forward to hearing your feedback on what we post here. The AB Engineering Team https://alphabravo.io ","date":"2021-11-15","objectID":"/posts/2021/welcome-to-the-blog/:0:0","tags":null,"title":"Welcome To The Blog","uri":"/posts/2021/welcome-to-the-blog/"}]